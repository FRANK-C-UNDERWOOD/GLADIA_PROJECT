"""
ğŸ§  MPSMemoryEncoder: Agent Memory Representation with Matrix Product State (MPS)
Author: DOCTOR + æ­Œè•¾è’‚å¨… (2025)
Description:
This module uses a linear tensor network structure (MPS) to encode sequences of semantic tokens
or user interaction features into a compressed and learnable memory embedding.
"""

import torch
import torch.nn as nn
import numpy as np
from typing import List, Tuple

class IntegratedTN_MPS(nn.Module):
    def __init__(self, 
                 tn_embed_dim=32,
                 mps_input_dim=3,
                 mps_bond_dim=16,
                 mps_output_dim=64):
        super().__init__()
        
        # 1. TNå‹ç¼©æ¨¡å—
        self.tn_compressor = TripleCompressor(embed_dim=tn_embed_dim)
        
        # 2. ç»´åº¦è½¬æ¢é€‚é…å™¨ - ä¿®æ”¹ä¸ºæ›´åˆé€‚çš„ç»“æ„
        self.adapter = nn.Sequential(
            nn.Linear(tn_embed_dim**3, 128),
            nn.ReLU(),
            nn.LayerNorm(128),
            nn.Linear(128, tn_embed_dim * 3),  # è¾“å‡ºè¶³å¤Ÿå…ƒç´ ç»„æˆ(s,p,o)åºåˆ—
            nn.ReLU()
        )
        
        # 3. MPSè®°å¿†ç¼–ç å™¨
        self.mps_encoder = MPSMemoryEncoder(
            input_dim=mps_input_dim,
            feature_dim=tn_embed_dim,
            bond_dim=mps_bond_dim,
            output_dim=mps_output_dim
        )
        
        # 4. é…ç½®å‚æ•°
        self.tn_embed_dim = tn_embed_dim
        self.mps_input_dim = mps_input_dim
        
    def forward(self, triples: List[Tuple[str, str, str]]) -> torch.Tensor:
        # ç¬¬ä¸€é˜¶æ®µï¼šTNå‹ç¼©
        batch_tensors = []
        for triple in triples:
            tn_tensor = self.tn_compressor.compress_triplet(triple)  # (D, D, D)
            flat_tensor = tn_tensor.flatten()  # (DÂ³,)
            adapted = self.adapter(torch.tensor(flat_tensor).float())  # è¾“å‡ºä¸º (D*3)
            batch_tensors.append(adapted)
        
        # æ„å»ºMPSè¾“å…¥åºåˆ— (s,p,o)
        batch_size = len(triples)
        # å°†åˆ—è¡¨å †å å¹¶é‡å¡‘ä¸º (batch_size, seq_len=3, feature_dim)
        seq_input = torch.stack(batch_tensors).view(batch_size, self.mps_input_dim, self.tn_embed_dim)
        
        # ç¬¬äºŒé˜¶æ®µï¼šMPSè®°å¿†ç¼–ç 
        return self.mps_encoder(seq_input)

class TripleCompressor:
    """ğŸ”§ TNå‹ç¼©å™¨ - ä¿æŒä¸å˜"""
    def __init__(self, embed_dim=32):
        self.embed_dim = embed_dim
    
    def text_to_tensor(self, text: str) -> np.ndarray:
        byte_data = text.encode('utf-8', errors='replace')
        safe_length = min(len(byte_data), self.embed_dim)
        truncated = byte_data[:safe_length]
        vec = np.frombuffer(truncated, dtype=np.uint8)
        if len(vec) < self.embed_dim:
            vec = np.pad(vec, (0, self.embed_dim - len(vec)), constant_values=0)
        return vec.astype(np.float32)
    
    def compress_triplet(self, triple: Tuple[str, str, str]) -> np.ndarray:
        s_vec = self.text_to_tensor(triple[0])
        p_vec = self.text_to_tensor(triple[1])
        o_vec = self.text_to_tensor(triple[2])
        tensor = np.tensordot(s_vec, p_vec, axes=0)
        tensor = np.tensordot(tensor, o_vec, axes=0)
        return tensor

class MPSMemoryEncoder(nn.Module):
    """ğŸ§  MPSç¼–ç å™¨ - ä¿æŒä¸å˜"""
    def __init__(self, input_dim, feature_dim, bond_dim, output_dim):
        super().__init__()
        self.n_sites = input_dim
        self.feature_dim = feature_dim
        self.bond_dim = bond_dim
        self.mps_cores = nn.ParameterList([
            nn.Parameter(torch.randn(bond_dim, feature_dim, bond_dim)) for _ in range(input_dim)
        ])
        self.start = nn.Parameter(torch.randn(1, bond_dim))
        self.end = nn.Parameter(torch.randn(bond_dim, 1))
        self.fc = nn.Linear(1, output_dim)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B = x.shape[0]
        start = self.start.expand(B, -1).unsqueeze(1)
        result = torch.matmul(start, self._contract(x[:, 0, :], self.mps_cores[0]))
        for i in range(1, self.n_sites):
            result = torch.matmul(result, self._contract(x[:, i, :], self.mps_cores[i]))
        end = self.end.unsqueeze(0).expand(B, -1, -1)
        result = torch.matmul(result, end).view(B, 1)
        return self.fc(result)
    
    def _contract(self, feature_vec, mps_tensor):
        return torch.einsum('bf,dfr->bdr', feature_vec, mps_tensor)

# ä¿®å¤åçš„æµ‹è¯•å‡½æ•°
def test_integrated_system():
    print("\n=== ğŸ§ª TN-MPSé›†æˆç³»ç»Ÿæµ‹è¯• (ä¿®å¤ç‰ˆ) ===")
    
    # 1. åˆ›å»ºé›†æˆæ¨¡å‹
    model = IntegratedTN_MPS(
        tn_embed_dim=32,
        mps_input_dim=3,
        mps_bond_dim=16,
        mps_output_dim=64
    )
    
    # 2. æµ‹è¯•æ•°æ®
    test_triples = [
        ("çˆ±å› æ–¯å¦", "æå‡º", "ç›¸å¯¹è®º"),
        ("ç‰›é¡¿", "å‘ç°", "ä¸‡æœ‰å¼•åŠ›"),
        ("å›¾çµ", "å‘æ˜", "å›¾çµæœº")
    ]
    
    # 3. å‰å‘ä¼ æ’­
    memory_vectors = model(test_triples)
    
    # 4. éªŒè¯è¾“å‡º
    print("âœ… è¾“å‡ºå½¢çŠ¶:", memory_vectors.shape)  # åº”ä¸º (3, 64)
    print("âœ… æ•°å€¼èŒƒå›´: [{:.2f}, {:.2f}]".format(
        memory_vectors.min().item(),
        memory_vectors.max().item()
    ))
    
    # 5. å‚æ•°éªŒè¯
    print("\nğŸ” æ¨¡å—å‚æ•°ç»Ÿè®¡:")
    print(f"- TNå‹ç¼©å™¨: åµŒå…¥ç»´åº¦={model.tn_embed_dim}")
    print(f"- MPSç¼–ç å™¨: è¾“å…¥ç»´åº¦={model.mps_input_dim}, é”®ç»´åº¦={model.mps_encoder.bond_dim}")
    print(f"- è®°å¿†ç»´åº¦: {memory_vectors.shape[-1]}")
    
    # 6. æ¢¯åº¦æµ‹è¯•
    test_tensor = torch.randn(3, 64, requires_grad=True)
    loss = (memory_vectors - test_tensor).pow(2).mean()
    loss.backward()
    print("\nâœ… æ¢¯åº¦åå‘ä¼ æ’­æˆåŠŸ")
    
    print("\nğŸ§ª é›†æˆç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼")

if __name__ == "__main__":
    test_integrated_system()