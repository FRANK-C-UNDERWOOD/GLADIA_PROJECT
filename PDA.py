"""
ğŸ¯ğŸ¯ PredictiveDialogAgent.py
æ ¸å¿ƒï¼šä»¥é¢„æµ‹ç¼–ç ä¸º Agent ä¸»æ§æ¡†æ¶ï¼Œæ•´åˆæç¤ºè¯ä¸ deepseek æ¥å£ï¼Œå®ç°ç±»äººå¯¹è¯ä¸è®°å¿†æ§åˆ¶ã€‚
æ‰©å±•ï¼šæ”¯æŒå¤šè½®æ€ç»´é“¾ï¼ˆCoTï¼‰ç®¡ç†ï¼Œé€šè¿‡é“¾å¼è®°å½•ä¸å›æº¯æå‡é€»è¾‘è¿è´¯æ€§ã€‚
"""
import os
import torch
from sentence_transformers import SentenceTransformer
from openai import AsyncOpenAI
from collections import deque
from typing import List, Tuple, Dict, Union
import asyncio
from PredictiveCoding import PredictiveCodingAgent
from GMB import GraphMemoryBank
from DSAP import MemoryAnchorUpdater
from MPR import MemoryRetriever


class DialogHistoryBuffer:
    def __init__(self, max_len=10):
        self.history = deque(maxlen=max_len)

    def add(self, user_input: str, agent_response: str):
        self.history.append((user_input, agent_response))

    def context_text(self) -> str:
        return "\n".join([f"ä½ ï¼š{u}\nAIï¼š{a}" for u, a in self.history])

    def thought_chain(self) -> str:
        return "\n".join([f"æ­¥éª¤{i+1}ï¼šç”¨æˆ·æé—®ï¼š{u}\n        AI å›ç­”ï¼š{a}" for i, (u, a) in enumerate(self.history)])


class PredictiveDialogAgent:
    def __init__(self, tool_threshold=0.25, memory_threshold=0.10, deepseek_api_key=None):
        self.embedder = SentenceTransformer('paraphrase-MiniLM-L6-v2')
        self.dim = self.embedder.get_sentence_embedding_dimension()

        self.DSAP = MemoryAnchorUpdater()
        self.tool_trigger_threshold = tool_threshold
        self.memory_trigger_threshold = memory_threshold
        self.deepseek_api_key = deepseek_api_key

        self.dialog_buffer = DialogHistoryBuffer(max_len=10)

        gmb = GraphMemoryBank()

        self.pc = PredictiveCodingAgent(
            tn_embed_dim=32,
            mps_bond_dim=16,
            mps_output_dim=64,
            hidden_dim=128,
            memory_capacity=200,
            memory_threshold=memory_threshold,
            graph_memory_bank=gmb
        )

        self.pc.MPR = MemoryRetriever(memory_embeddings=[], memory_triples=[], similarity_threshold=0.7)

        self.client = AsyncOpenAI(
            base_url="https://api.deepseek.com/v1",
            api_key=deepseek_api_key
        )

    def save_memory_to_disk(self, path_prefix="dialog_memory"):
        self.pc.graph_memory.save_all(path_prefix)

    def load_memory_from_disk(self, path_prefix="dialog_memory"):
        if not os.path.exists(f"{path_prefix}.graph.json") or not os.path.exists(f"{path_prefix}.pt"):
            print(f"[âš ï¸] æœªå‘ç°æŒä¹…åŒ–è®°å¿†æ–‡ä»¶ '{path_prefix}', è·³è¿‡åŠ è½½ã€‚")
            return
        self.pc.load_memory(path_prefix)
        print(f"[âœ…] æˆåŠŸåŠ è½½è®°å¿†ï¼š{path_prefix}")

    def embed(self, text: str) -> torch.Tensor:
        vec = self.embedder.encode(text, normalize_embeddings=True)
        return torch.tensor(vec).unsqueeze(0)

    def enhanced_memory_retrieval(self, query_text: str) -> List[Tuple[str, str, str]]:
        return self.pc.recall_memory(("æŸ¥è¯¢", "å†…å®¹", query_text))

    def get_memory_stats(self) -> Dict[str, float]:
        return self.pc.memory_stats()

    async def extract_triplet(self, text: str) -> Union[Tuple[str, str, str], None]:
        prompt = (
            f"è¯·ä»ä¸‹å¥ä¸­æŠ½å–å‡ºæœ€é‡è¦çš„ä¸€ç»„ä¸‰å…ƒç»„ï¼ˆä¸»ä½“, å…³ç³», å®¢ä½“ï¼‰ï¼Œ"
            f"å¹¶ä»¥ [ä¸»ä½“], [å…³ç³»], [å®¢ä½“] çš„æ ¼å¼è¿”å›ï¼Œä¸è¦è§£é‡Šï¼š\n{text}"
        )
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–åŠ©æ‰‹ã€‚"},
                    {"role": "user", "content": prompt}
                ]
            )
            content = response.choices[0].message.content.strip()
            if ',' in content:
                parts = [x.strip().strip('[]') for x in content.split(',')]
                if len(parts) == 3:
                    return tuple(parts)
        except Exception:
            return None
        return None

    async def chat(self, prompt: str) -> str:
        anchors_block = ""
        anchor_triples = []
        for k in self.DSAP.get_shared_anchors().keys():
            parts = k.split("::")
            if len(parts) == 3:
                anchor_triples.append((parts[0], parts[1], parts[2]))
        if anchor_triples:
            anchors_block = "\nä½ è®¾ç½®çš„èº«ä»½/åå¥½å…³é”®ç‚¹å¦‚ä¸‹ï¼š\n" + "\n".join(
                [f"{s} â€” {p} â€” {o}" for s, p, o in anchor_triples])
            memory_embedding = self.pc.encode_input(anchor_triples)
        else:
            memory_embedding = torch.zeros(1, 64)

        context_block = f"\nä»¥ä¸‹æ˜¯ä½ ä¸ç”¨æˆ·çš„æœ€è¿‘å‡ è½®å¯¹è¯ï¼š\n{self.dialog_buffer.context_text()}" if self.dialog_buffer.history else ""
        chain_block = f"\nä»¥ä¸‹æ˜¯ä½ ä»¬çš„æ€ç»´æ¨ç†è¿‡ç¨‹ï¼š\n{self.dialog_buffer.thought_chain()}" if self.dialog_buffer.history else ""

        knowledge_triples = self.enhanced_memory_retrieval(prompt)
        knowledge = "\n".join([f"{s} â€” {p} â€” {o}" for s, p, o in knowledge_triples]) or "ï¼ˆæ— ç›¸å…³è®°å¿†ï¼‰"
        vec_summary = ", ".join([f"{v:.2f}" for v in memory_embedding[0].detach().numpy()[:5]])

        if knowledge_triples:
            _, avg_error = self.pc.forward(knowledge_triples)
            prediction_summary = f"é¢„æµ‹ç¼–ç è¯¯å·®: {avg_error:.3f}"
        else:
            avg_error = 0.0
            prediction_summary = "æ— é¢„æµ‹ç¼–ç æ•°æ®"

        system_prompt = (
            "ä½ æ˜¯æ­Œè•¾è’‚å¨…å®éªŒå®¤çš„ç§‘ç ”åŠ©æ‰‹ã€‚\n"
            f"{anchors_block}{context_block}{chain_block}\n"
            f"ä»¥ä¸‹æ˜¯ä¸è¾“å…¥ç›¸å…³çš„è®°å¿†å†…å®¹ï¼š\n{knowledge}\n\n"
            f"å‘é‡æ‘˜è¦ï¼š{vec_summary}\n\n"
            f"è®°å¿†ç³»ç»ŸçŠ¶æ€: {prediction_summary}\n\n"
            "ã€ä»»åŠ¡è¦æ±‚ã€‘è¯·ç”¨è‡ªç„¶è¯­è¨€å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œå‹¿é‡å¤è¾“å‡ºå‘é‡ä¿¡æ¯ã€‚\n"
            "å¦‚æœªæ‰¾åˆ°ç›¸å…³çŸ¥è¯†ï¼Œå¯é€»è¾‘æ¨ç†ã€‚"
        )

        full_output = ""
        try:
            response = await self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": prompt}
                ],
                stream=True,
            )
            async for chunk in response:
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                full_output += content
        except Exception as e:
            print(f"\n[âŒ] API è°ƒç”¨å¤±è´¥: {str(e)}")
            return "æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶å‡ºç°é—®é¢˜"

        _, err = self.pc.forward([("ç”¨æˆ·", "è¯¢é—®", prompt)])
        if err > self.memory_trigger_threshold:
            triple = await self.extract_triplet(prompt)
            if triple and any(triple):
                vec = self.embed(prompt)
                self.pc.graph_memory.add_triplet(triple, vec, error=err)
                print(f"\nğŸ“¥ è®°å¿†æ›´æ–°ï¼š{triple}")

        self.dialog_buffer.add(prompt, full_output)
        return full_output

    def chat_with_user(self, user_input: str) -> str:
        return asyncio.run(self.chat(user_input))

    def clear_memory_systems(self):
        self.pc.graph_memory.graph_nodes.clear()
        self.pc.graph_memory.graph_edges.clear()
        self.dialog_buffer.history.clear()
        print("ğŸ§¹ æ‰€æœ‰å›¾ç»“æ„è®°å¿†å’Œå¯¹è¯å†å²å·²æ¸…ç©º")

    def get_comprehensive_memory_stats(self) -> Dict[str, any]:
        stats = self.get_memory_stats()
        stats.update({
            'dialog_history_length': len(self.dialog_buffer.history),
            'anchor_count': len(self.DSAP.get_shared_anchors()),
        })
        return stats


# ==================== å¯åŠ¨å…¥å£ ====================
if __name__ == '__main__':
    print("=== ğŸ§  PDA é¢„æµ‹å¯¹è¯ç³»ç»Ÿå¯åŠ¨ ===")
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "ä½ çš„API_KEY")

    agent = PredictiveDialogAgent(deepseek_api_key=DEEPSEEK_API_KEY)
    agent.load_memory_from_disk("dialog_memory")

    async def main_loop():
        while True:
            q = input("\nä½ ï¼š")
            if q.strip().lower() in ["exit", "é€€å‡º", "quit"]:
                break
            print("AIï¼š", end="", flush=True)
            await agent.chat(q)

        print("\nğŸ’¾ æ­£åœ¨ä¿å­˜è®°å¿†...")
        agent.save_memory_to_disk("dialog_memory")

    asyncio.run(main_loop())
